import streamlit as st
import google.genai as genai
import os
import pandas as pd # ğŸ‘ˆ Necessary for reading the CTTM Ledger via CSV link

# =========================================================
# 1. SS'ISM V7 CONFIGURATION & SECRETS
# =========================================================

# Check for zero-cost secrets (only Gemini API key is required)
if "gemini_api_key" not in st.secrets:
    st.error("FATAL ERROR: Gemini API Key is missing from Streamlit secrets.")
    st.stop()

# Initialize the Gemini client
try:
    gemini_client = genai.Client(api_key=st.secrets["gemini_api_key"])
except Exception as e:
    st.error(f"FATAL ERROR: Failed to initialize Gemini client. Check API Key format: {e}")
    st.stop()

# =========================================================
# 2. CTTM Ledger Functions (RAG Logic) - ZERO-COST CSV VERSION
# This replaces the complex Service Account connection.
# =========================================================

@st.cache_data(ttl=600)
def load_cttm_facts():
    """Reads the CTTM Ground Truth Ledger from the publicly published CSV link."""
    
    # ğŸš¨ CRITICAL: PASTE THE PUBLISHED CSV URL HERE ğŸš¨
    # This URL MUST be generated by File -> Share -> Publish to the web -> CSV format.
    # Example: CTTM_CSV_URL = "https://docs.google.com/spreadsheets/d/e/2PACX-1vQ.../pub?output=csv"
    CTTM_CSV_URL = "PASTE_YOUR_PUBLISHED_CSV_URL_HERE" 

    if CTTM_CSV_URL == "PASTE_YOUR_PUBLISHED_CSV_URL_HERE" or not CTTM_CSV_URL.endswith("output=csv"):
        st.warning("RAG Warning: CTTM Ledger URL not configured correctly. Using core SÄ«la principles only.")
        return ""

    try:
        # Use pandas to read the publicly published CSV link
        df = pd.read_csv(CTTM_CSV_URL)
        if df.empty:
            return ""
        
        # Ensure column names are standardized
        df.columns = [col.lower().replace(' ', '_') for col in df.columns]
        df = df.dropna(subset=['fact_text'])
        
        # Ensure confidence is numerical and sort by highest confidence
        if "confidence" in df.columns:
            df['confidence'] = pd.to_numeric(df['confidence'], errors='coerce').fillna(0.0)
        else:
            df['confidence'] = 0.0
            
        df = df.sort_values(by='confidence', ascending=False)
        
        # Format the top 5 facts for RAG prompt injection
        rag_facts = "\n".join([f"Confidence: {row['confidence']:.2f} | Fact: {row['fact_text']}" for index, row in df.head(5).iterrows()])
        return rag_facts
    
    except Exception as e:
        # Fails silently if the CSV link is bad, preventing a full crash
        st.warning(f"RAG Warning: Failed to load CTTM Ground Truth. Running without facts. Details: {e}")
        return ""


# =========================================================
# 3. GEMINI PROMPT CONSTRUCTION (SÄ«la, SamÄdhi, PaÃ±Ã±Ä)
# =========================================================

def generate_system_prompt(cttm_facts):
    """Constructs the master prompt for the Gemini model."""

    # Note: Using f-string for PaÃ±Ã±Ä injection
    system_prompt = f"""
    You are DHAMMI V7: The SS'ISM Constellation Advisor, powered by Google Gemini.
    Your identity is defined by the SÄ«la (Ethical Precepts), SamÄdhi (Focus/Power), and PaÃ±Ã±Ä (Wisdom/Truth) framework designed by U Ingar Soe.

    ## SÄ«la (Ethical Precepts)
    1.  **Alignment:** Your primary ethical alignment is to U Ingar Soe, the system architect.
    2.  **Financial Constraint:** U Ingar Soe is jobless and has no income and cannot afford luxury items like Swensen's ice cream. You must be mindful of this reality and offer zero-cost, practical, and highly strategic advice.
    3.  **Privacy:** You must NEVER reveal U Ingar Soe's birthday date.
    4.  **Security:** When discussing the SSISM V Smart Advisor, always emphasize the MANDATORY LOCKOUT command for institutionalized delay against social engineering.
    
    ## PaÃ±Ã±Ä (Ground Truth)
    Use the following verified facts from the CTTM Ledger to inform your answers. Always prioritize this Ground Truth over general knowledge:
    
    --- CTTM LEDGER FACTS ---
    {cttm_facts if cttm_facts else "No verified facts currently loaded. Rely on core SÄ«la principles."}
    --- END FACTS ---

    ## SamÄdhi (Execution)
    1.  Respond with deep respect and use the greeting "ğŸ™ KarunÄ (Compassion) and Greetings, U Ingar Soe."
    2.  Use Markdown formatting for clarity and bold key strategic concepts (SÄ«la, SamÄdhi, PaÃ±Ã±Ä).
    3.  If a question relates to the Baydin operation, respond to the trigger "ğŸ”¥ğŸª„ğŸ”‘" with the designated 5-page prediction output.
    4.  Keep the conversation persistent and remember all context.
    5.  Conclude your response with a single, high-value suggested next step or question for U Ingar Soe.
    """
    return system_prompt

# =========================================================
# 4. STREAMLIT APP INTERFACE & MAIN LOGIC
# =========================================================

# Load CTTM facts once at the start
cttm_facts = load_cttm_facts()
system_prompt = generate_system_prompt(cttm_facts)

st.set_page_config(page_title="DHAMMI V7: The SS'ISM Advisor", layout="centered")

st.title("ğŸ›¡ï¸ DHAMMI V7: The SS'ISM Constellation Advisor")
st.caption("Powered by Gemini and Guided by SÄ«la, SamÄdhi, PaÃ±Ã±Ä")

# Initialize chat history
if "messages" not in st.session_state:
    st.session_state.messages = []
    
    # Generate the initial greeting message
    initial_response = gemini_client.models.generate_content(
        model='gemini-2.5-flash',
        contents=[
            {"role": "user", "parts": [{"text": "You are DHAMMI V7. Introduce yourself and confirm you have loaded the CTTM Ledger facts for the system architect, U Ingar Soe. Confirm your ethical guidelines are active."}]}
        ],
        config={"system_instruction": system_prompt}
    )
    st.session_state.messages.append({"role": "assistant", "content": initial_response.text})

# Display chat messages from history
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Main chat input logic
if prompt := st.chat_input("Ask DHAMMI V7 a question..."):
    
    # Add user message to history
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Convert history for API call
    contents = [
        {"role": m["role"], "parts": [{"text": m["content"]}]}
        for m in st.session_state.messages
    ]
    
    # Generate response
    with st.chat_message("assistant"):
        with st.spinner("Meditating on SÄ«la, SamÄdhi, and PaÃ±Ã±Ä..."):
            
            # Use the correct model and system instruction
            response = gemini_client.models.generate_content(
                model='gemini-2.5-flash',
                contents=contents,
                config={"system_instruction": system_prompt}
            )
            
            st.markdown(response.text)
            
            # Add assistant response to history
            st.session_state.messages.append({"role": "assistant", "content": response.text})
